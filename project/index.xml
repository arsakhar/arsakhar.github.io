<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | </title>
    <link>https://arsakhar.github.io/project/</link>
      <atom:link href="https://arsakhar.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 28 Jun 2020 22:19:19 -0700</lastBuildDate>
    <image>
      <url>https://arsakhar.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>Projects</title>
      <link>https://arsakhar.github.io/project/</link>
    </image>
    
    <item>
      <title>NeuroRiderVR</title>
      <link>https://arsakhar.github.io/project/vr-bike/</link>
      <pubDate>Sun, 28 Jun 2020 22:19:19 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/vr-bike/</guid>
      <description>&lt;h3 id=&#34;background&#34;&gt;Background&lt;/h3&gt;
&lt;p&gt;In a recent blog post (&lt;a href=&#34;https://arsakhar.github.io/blog/commercial-locomotion/&#34;&gt;Locomotion in VR&lt;/a&gt;), I detailed some of the current technology that exists for locomotion in VR. Until recently, VirZoom was the only virtual reality bike on the market. However, at CES 2019, Nordic Track announced the release of their VR bike. This is great news for VR as many of the bigger companies are starting to see the value of VR in exercise and entertainment. The target audience for both VR bikes are generally younger to middle aged adults with a focus on entertainment, exercise, and gaming.&lt;/p&gt;
&lt;p&gt;In our academic research lab, we focus on the applications of VR to rehabilitation in an older adult population. At the time of our VR biking study in older adults, the Nordic Track VR bike did not exist. While the VirZoom bike was a possibility, we did have several concerns. Turning in VR with the VirZoom bike is achieved by leaning one&amp;rsquo;s body in the intended direction of movement. VirZoom claims that this is a more natural method of turning that minimizes motion sickness and improves user safety while riding the VR bike.&lt;/p&gt;
&lt;p&gt;When we tried the VirZoom bike in our lab, our initial impressions were that it was a bit unnatural. While it&amp;rsquo;s possible that users would adjust to this over time, we thought the older adult population might do better on a bike with a turning mechanism that more closely resembled that of an actual bike. Furthermore, we hypothesized that requiring older adults to lean might introduce an additional safety risk, particularly in those with poor postural stability.&lt;/p&gt;
&lt;h3 id=&#34;objective&#34;&gt;Objective&lt;/h3&gt;
&lt;p&gt;Our objective was to develop a custom bike that would allow for turning in VR by the same mechanism of a regular bike, rotating the handlebar clockwise or counter-clockwise. Moreover, we wanted our design to include digital control of the pedal resistance. This would allow for real-time manipulation of pedal resistance when participants were going up or down hills in a VR environment or as a means to increase a participants exertion during exercise.&lt;/p&gt;
&lt;h3 id=&#34;design-requirements&#34;&gt;Design Requirements&lt;/h3&gt;
&lt;p&gt;The primary design requirements for our custom bike were stability, comfort, and ease of access for older adults. The bike was required to accommodate adults of different heights, trunk, leg, and arm lengths, and the turning and braking mechanisms for our bike were to be similar to that of a road bike. Furthermore, the bike was to be designed such that the pedal resistance could be manipulated from the computer in real-time. Finally, the bike should be able to interface with a computer and provide speed, turning, temperature, and braking feedback.&lt;/p&gt;
&lt;h3 id=&#34;mechanical-design&#34;&gt;Mechanical Design&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/img/vr-bike/cad-model.png&#34; alt=&#34;Example image&#34;&gt;&lt;/p&gt;
&lt;p&gt;The first step in the design process was to create a model of the bike using SolidWorks, a 3D CAD software. We then had the bike fabricated at our University&#39;s (USC) machine shop. The drivetrain, handlebar, and seat post frames were welded and constructed out of aluminum to reduce weight and cost. All other components, including the leg stabilizers and gearing mechanism within the drive train were bolt-mounted for ease of assembly/disassembly. The bike was comprised of adjustable angled telescopic seat and handlebar posts. The handlebar was mounted to a ball-bearing to allow approximately 150 degrees rotation. A brass set screw was used to manually adjust the resistance to turning. The bike consisted of a double gear reduction such that the output torque was approximately 11.5% of the input torque. A freewheel sprocket was used to allow for coasting. Radially oriented electromagnets were mounted concentrically inside a flywheel to allow for varying of pedal resistance using the principles of eddy current braking.&lt;/p&gt;
&lt;h3 id=&#34;electrical-design&#34;&gt;Electrical Design&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/img/vr-bike/circuit.png&#34; alt=&#34;Example image&#34;&gt;
&lt;img src=&#34;https://arsakhar.github.io/img/vr-bike/schematic.png&#34; alt=&#34;Example image&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Speed Detection&lt;/b&gt;&lt;br&gt;
Speed detection was achieved using a custom laser-cut acrylic encoder wheel and Omron EE-SV3-D transmissive optical sensor. The optical sensor consisted of a photodiode (light emitter) and phototransistor (light detector). A standard 220 Ω resistor was connected in series between the 5V supply voltage and anode to keep the forward current within its specified current rating. The phototransistor was configured as a common collector (CC) phototransistor circuit. A 10 kΩ load resistor was added between ground and the emitter to configure the phototransistor for switch mode (), where the output was either OFF or ON based on the absence or presence of light, respectively. It should be noted that a 5 kΩ load resistor is typically adequate for switch mode and will have a faster response time to changes in light intensity compared to a 10 kΩ load resistor. The output signal from the collector was connected to a digital pin on the Arduino.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Turning and Brake Sensing&lt;/b&gt;&lt;br&gt;
A linear, rotatory 10 kΩ potentiometer and dual-axis XY joystick module were used for turning and brake sensing, respectively. Both sensors were configured as voltage dividers, outputting a voltage between 0V and the 5V supply voltage based on the rotation of the knob or the deflection of the joystick.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Temperature Sensor&lt;/b&gt;&lt;br&gt;
A TMP36 semi-conductor temperature sensor was used to monitor heat dissipation at the electromagnet coils. A 100 nF capacitor was added between the output pin and ground for each sensor to dampen signal fluctuations due to mechanical bounce, electrical noise, and electromagnetic interference. The output pin on each sensor was connected to a unique analog pin on the Arduino.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Pedal Resistance&lt;/b&gt;&lt;br&gt;
Pulse width modulation (PWM) and a RFP30N06LE MOSFET transistor were used to manipulate pedal resistance. An HCPL-7710 opto-isolator was placed between the Arduino’s digital PWM signal and the MOSFET to electrically isolate the MOSFET from the Arduino. Indeed, the high PWM signal frequency (490 Hz) resulted in frequent switching of the MOSFET which caused undesirable noise during signal sampling. The PWM signal was connected to pin 2 of the opto-isolator. Arduino’s 5V power supply and ground were connected to pins 1 and 4 of the opto-isolator. 12V from an external variable power supply was regulated down to 5V via a L7805ABV linear voltage regulator and connected to pin 5 of the opto-isolator. Pin 8 of the opto-isolator was connected to ground of the external power supply. A 100 nF capacitor was connected between pins 1 and 4 and 5 and 8 of the opto-isolator in accordance with datasheet recommendations. Similarly, a 330 nF and 100 nF capacitor were connected between ground and the voltage regulator input and output pins, respectively.&lt;/p&gt;
&lt;p&gt;The opto-isolated PWM signal at pin 6 of the opto-isolator was connected to the gate pin on the MOSFET. A 47 Ω gate resistor was placed in series with pin 6 of the opto-isolator and the MOSFET gate to limited ringing and electrical noise due to parasitic inductance. A 10 kΩ pull-down resistor was connected in series between ground and the MOSFET gate to pull the voltage down and turn the MOSFET off in the absence of a PWM input signal. The MOSFET source was connected to the source of the MOSFET. One electromagnet lead was connected to the source of the MOSFET, while the other was connected to 12V power from the external power supply. An 1N5817 Schottky diode was connected in parallel with the leads of the electromagnet to prevent flyback when current to the electromagnet is interrupted. A 2000 µF electrolytic capacitor and 100 nF capacitor were connected in parallel across 12V power and ground from the external power supply to smooth out the input voltage to the electromagnet.&lt;/p&gt;
&lt;h3 id=&#34;arduino-programming&#34;&gt;Arduino Programming&lt;/h3&gt;
&lt;p&gt;The temperature, turning, and braking sensors were connected to analog input pins, the speed sensor was connected to a digital input pin, and pedal resistance was provided by a PWM output pin on the Arduino Uno. The Arduino Uno connected to the PC via a USB A-Male to B-Male cable. A standard Arduino Uno driver was used to establish the connection as a COM port on the PC. The baud rate was set to 1,000,000 for serial port communication.&lt;/p&gt;
&lt;p&gt;The ResponsiveAnalogRead library was used to reduce noise associated with the analog input signals for the brake, turning, and temperature sensors. All analog sensor values were updated within the main loop. For the speed sensor, an interrupt trigger was used to track when the state of the optical sensor changed. A digital pin reads HIGH(1) when the voltage signal is &amp;gt; 3.0V and LOW(0) when the voltage is &amp;lt; 3.0V on a 5V Arduino board.&lt;/p&gt;
&lt;p&gt;When determining speed, the Arduino code is structured to count the number of times the digital interrupt is triggered over a specified sampling period of 50ms. A digital interrupt is triggered when the encoder wheel spoke enters and leaves the light path. The number of interrupts over this 50 ms period is then converted to a speed using the following formula:&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Interrupt Count&lt;/b&gt; (counts/s) = Interrupt Count * (1 / 50 ms) * (1 / 1000s)
&lt;br&gt;&lt;b&gt;Speed (m/s)&lt;/b&gt; = (2 * pi * Flywheel Radius (m)) / (Spokes per Revolution) * (Interrupt Count)&lt;/p&gt;
&lt;p&gt;Note: In practice, an additional fudge factor may be necessary to get the perceived pedal speed to correctly match the speed at which the user moves through the virtual environment&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Encoder Wheel&lt;/b&gt;&lt;br&gt;
The encoder wheel was mounted to the output shaft of the flywheel and centered within the gap of the optical sensor. For the encoder wheels, we laser cut 6 different variations out of acrylic. These variations included 2, 4, 8, 16, 32, 64, and 128 spokes. For each wheel, we evaluated 2 conditions: aliasing and a ceiling effect. Aliasing occurs when the encoder wheel rotates faster than the optical sensor can react to the light change or the Arduino can trigger the digital interrupt pin. With a 10K load resistor, the response time of the phototransistor is approximately 100 µs. With the Arduino, the interrupt service routine theoretically executes every .625 µS based on a clock speed of 16 MHz.&lt;/p&gt;
&lt;p&gt;However, in practice, this is often much slower due to factors such as code overhead and often results in an interrupt frequency on the scale of milliseconds, which likely makes it the rate limiting step. The ceiling effect occurs when the resolution of the encoder wheel is too small to capture the actual pedal speed. In other words, after a certain pedal speed, the number of spokes that pass through the optical sensor gets capped over a 50 ms period. During testing, we found that the 2,4,8, and 16 spoke encoder wheels experienced the ceiling effect while the 128 spoke encoder wheel had significant aliasing at normal pedal speeds. We found the 64 spoke encoder wheel to be the best option as we found no aliasing and no ceiling effect, even at very high pedal speeds.&lt;/p&gt;
&lt;h3 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h3&gt;
&lt;p&gt;The bike design was a team effort with significant contributions from Roshan Ravichandran (lead mechanical design) and Delian Delev (lead electrical design). Additional design support was provided by Chintan Raja and Vincent Yang. Electromagnet guidance was provided by Andrew Bushnell. Special thanks to Don Wiggins and the USC machine shop for manufacturing the bike.&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>FitViz</title>
      <link>https://arsakhar.github.io/project/fitviz/</link>
      <pubDate>Fri, 05 Jun 2020 23:32:49 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/fitviz/</guid>
      <description>&lt;div class=&#34;article-container-nav&#34;&gt;

	&lt;nav&gt;
	
		&lt;div class=&#34;sticky-scroll navtab-position nav flex-row nav-pills&#34; id=&#34;nav-tab&#34; role=&#34;tablist&#34;&gt;
			&lt;br&gt;
			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link active&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab11&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;true&#34;&gt;Overview&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab12&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Skills&lt;/a&gt;

			
			
		&lt;/div&gt;
	&lt;/nav&gt;

	&lt;div class=&#34;article-style &#34;&gt;
		&lt;div class=&#34;tab-content&#34; id=&#34;1&#34;&gt;
			






&lt;div class=&#34;tab-pane fade show active&#34; id=&#34;tab11&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName1&#34;&gt;

	&lt;h4 id=&#34;overview&#34;&gt;Overview&lt;/h4&gt;
&lt;p&gt;FitViz is a monitoring client designed for fitness enthusiasts and gamers. It supports real-time visualization and tracking of health and fitness data sent from ANT+ devices. FitViz also provides networking capabilities allowing game developers to easily integrate ANT+ devices, such as bike trainers and heart rate monitors, into their own game.&lt;/p&gt;
&lt;p&gt;The idea behind this project was to provide a standalone PC application that allowed me to interface with a Wahoo Kickr Snap bike trainer. In particular, I was interested in obtaining real-time measurements of speed and power from the Wahoo Kickr to be used as an input controller for a VR game.&lt;/p&gt;
&lt;h4 id=&#34;about-ant&#34;&gt;About ANT+&lt;/h4&gt;
&lt;p&gt;ANT+ is a wireless sensor network technology that allows you to moniter data broadcast from ANT+ capable devices. Fitness equipment, bike trainers, heart rate monitors, and blood pressure monitors are just a few of the many devices supported within the ANT+ ecosystem. Data broadcast from ANT+ devices is standardized based on the type of data being sent. ANT+ refers to a data type as a device profile. An ANT+ device can broadcast data associated with multiple device profiles. For example, the Wahoo Kickr Snap broadcasts bicycle power and fitness equipment data.&lt;/p&gt;
&lt;p&gt;FitViz can be downloaded at: &lt;a href=&#34;https://github.com/arsakhar/FitViz&#34;&gt;https://github.com/arsakhar/FitViz&lt;/a&gt;&lt;/p&gt;


&lt;/div&gt;











&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab12&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName2&#34;&gt;

	&lt;h4 id=&#34;technical-skills&#34;&gt;Technical Skills&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;ANT+ Communication&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Communicating with and reading sensor information from ANT+ devices&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Exporting Data&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Writing data to UDP port&lt;/li&gt;
      &lt;li&gt;Saving data to CSV&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Visualization&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Developing GUI&lt;/li&gt;
      &lt;li&gt;Displaying sensor values in real-time&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Training a CNN to classify MRI scans according to anatomical level in the brain&lt;/li&gt;
    &lt;/ul&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;packages&#34;&gt;Packages&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;PyQt5&lt;/li&gt;
  &lt;li&gt;LibAnt&lt;/li&gt;
&lt;/ol&gt;


&lt;/div&gt;




		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>NeuroFlow</title>
      <link>https://arsakhar.github.io/project/neuroflow/</link>
      <pubDate>Fri, 05 Jun 2020 23:32:49 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/neuroflow/</guid>
      <description>&lt;div class=&#34;article-container-nav&#34;&gt;

	&lt;nav&gt;
	
		&lt;div class=&#34;sticky-scroll navtab-position nav flex-row nav-pills&#34; id=&#34;nav-tab&#34; role=&#34;tablist&#34;&gt;
			&lt;br&gt;
			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link active&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab11&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;true&#34;&gt;Overview&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab12&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Skills&lt;/a&gt;

			
			
		&lt;/div&gt;
	&lt;/nav&gt;

	&lt;div class=&#34;article-style &#34;&gt;
		&lt;div class=&#34;tab-content&#34; id=&#34;1&#34;&gt;
			






&lt;div class=&#34;tab-pane fade show active&#34; id=&#34;tab11&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName1&#34;&gt;

	&lt;h4 id=&#34;overview&#34;&gt;Overview&lt;/h4&gt;
&lt;p&gt;NeuroFlow is an imaging tool that allows neuroscientists and clinicians to analyze cerebral flow dynamics in the brain. The continuous circulation of cerebrospinal fluid (CSF) and cerebral blood flow (CBF) is key to the health of our central nervous system. When CSF and CBF dynamics in the brain become dysregulated, pathophysiological states can occur. As such, cerebral flow dynamics may be an important biomarker for identifying meaningful alterations in neurological diseases.&lt;/p&gt;
&lt;p&gt;Phase-contrast MRI (PC-MRI) is a validated, non-invasive MRI imaging technique, allowing rapid measurements of CSF and CBF flow in the brain. NeuroFlow provides a user-friendly interface for neuroscientists and clinicians to analyze PC-MRI images and extract measurements associated with cerebral flow dynamics. Moreover, NeuroFlow provides numerous tools to help user&amp;rsquo;s quickly, accurately, and painlessly analyze flow data.&lt;/p&gt;
&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe width=&#34;560&#34; height=&#34;315&#34; src=&#34;https://www.youtube.com/embed/VVsJvCKQgo0&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;NeuroFlow can be downloaded at: &lt;a href=&#34;https://github.com/arsakhar/NeuroFlow&#34;&gt;https://github.com/arsakhar/NeuroFlow&lt;/a&gt;&lt;/p&gt;


&lt;/div&gt;











&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab12&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName2&#34;&gt;

	&lt;h4 id=&#34;technical-skills&#34;&gt;Technical Skills&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Reading and Loading DICOMDIR&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Storing Patient, Study, Series, Sequence, and Image information entities of the DICOM data model in accordance with the NEMA DICOM standard.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Image Segmentation&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Providing tools for users to segment and label ROI’s on an image&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Visualization&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Developing GUI&lt;/li&gt;
      &lt;li&gt;Displaying Graphs, Images, Tables, and DICOM Metadata&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Image Classification&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Training a CNN to classify MRI scans according to anatomical level in the brain&lt;/li&gt;
    &lt;/ul&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;packages&#34;&gt;Packages&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;PyQt5&lt;/li&gt;
  &lt;li&gt;NumPy&lt;/li&gt;
  &lt;li&gt;Pandas&lt;/li&gt;
  &lt;li&gt;PyQtGraph&lt;/li&gt;
  &lt;li&gt;PIL&lt;/li&gt;
  &lt;li&gt;CV2&lt;/li&gt;
  &lt;li&gt;Torch&lt;/li&gt;
  &lt;li&gt;TorchVision&lt;/li&gt;
  &lt;li&gt;Matplotlib&lt;/li&gt;
  &lt;li&gt;Pydicom&lt;/li&gt;
&lt;/ol&gt;


&lt;/div&gt;




		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Ranking College Football Coaches</title>
      <link>https://arsakhar.github.io/project/ncaaf-coaches/</link>
      <pubDate>Fri, 05 Jun 2020 23:32:49 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/ncaaf-coaches/</guid>
      <description>&lt;div class=&#34;article-container-nav&#34;&gt;

	&lt;nav&gt;
	
		&lt;div class=&#34;sticky-scroll navtab-position nav flex-row nav-pills&#34; id=&#34;nav-tab&#34; role=&#34;tablist&#34;&gt;
			&lt;br&gt;
			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link active&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab11&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;true&#34;&gt;Overview&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab12&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Skills&lt;/a&gt;

			
			
		&lt;/div&gt;
	&lt;/nav&gt;

	&lt;div class=&#34;article-style &#34;&gt;
		&lt;div class=&#34;tab-content&#34; id=&#34;1&#34;&gt;
			






&lt;div class=&#34;tab-pane fade show active&#34; id=&#34;tab11&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName1&#34;&gt;

	&lt;h4 id=&#34;backgroundbr&#34;&gt;Background&lt;br&gt;&lt;/h4&gt;
&lt;p&gt;I’m an avid college football fan with a keen interest in data science. For my first dive into data science, I will use some basic statistics and data manipulation to evaluate coaching in college football.
&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;h4 id=&#34;defining-the-problembbr&#34;&gt;Defining the Problem&lt;/b&gt;&lt;br&gt;&lt;/h4&gt;
&lt;p&gt;Football is the crown jewel and breadwinner in any collegiate athletic department. A successful football program is not only a source of revenue for the university, it is also a powerful marketing tool, as studies have shown winning to be associated with increased donations, academic reputation, and lower acceptance rates. Therefore, it’s no surprise that coaches wield significant power and lofty salaries.&lt;/p&gt;
&lt;p&gt;Coaches also carry the burden of high expectations, poor job security, and an average tenure of only 3.8 years. While the decision to hire or fire a coach is a multi-factorial process, it is primarily driven by their record. Consequently, a coach’s value, particularly with donors and fans, is inextricably tied to their performance on the field.&lt;/p&gt;
&lt;p&gt;However, wins and losses are driven by several factors, many of which are outside a coach’s control, including conference affiliation, prestige, location, historical success, and financial investment from the university. As such, a coach’s record is not the most objective way to assess their value. In this project we will set out to define an approach for evaluating coaches while minimizing the impact of these confounding factors. Let&amp;rsquo;s get started!&lt;/p&gt;
&lt;p&gt;Continue reading on Medium: &lt;a href=&#34;https://medium.com/@arsakhare87/using-data-science-to-evaluate-recruiting-and-player-development-in-college-football-a8c5c5cd447d&#34;&gt; Using Data Science to Evaluate Recruiting and Player Development in College Football &lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Code can be downloaded at: &lt;a href=&#34;https://github.com/arsakhar/NCAAF&#34;&gt;&lt;a href=&#34;https://github.com/arsakhar/NCAAF&#34;&gt;https://github.com/arsakhar/NCAAF&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab12&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName2&#34;&gt;

	&lt;h4 id=&#34;technical-skills&#34;&gt;Technical Skills&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Data Visualization&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Histograms, scatterplots, qq plots, boxplots&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Data Scraping&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Scraping data across 4 different websites&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Data Cleaning&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Removing empty rows in dataframe&lt;/li&gt;
      &lt;li&gt;Removing non-numeric / nan rows in dataframe&lt;/li&gt;
      &lt;li&gt;Filtering dataframe by a specific keyword or attribute&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Data Manipulation&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Joining dataframes&lt;/li&gt;
      &lt;li&gt;Transforming values on a dataframe column&lt;/li&gt;
      &lt;li&gt;Applying a function elementwise on a dataframe column&lt;/li&gt;
      &lt;li&gt;Filtering dataframe based on a specific attribute or keyword&lt;/li&gt;
      &lt;li&gt;Aggregating on a dataframe column (standard deviation, mean, min, max, sum, count)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;li&gt;Statistical Analysis&lt;/li&gt;
    &lt;ul style=&#34;list-style-type:square;&#34;&gt;
      &lt;li&gt;Shapiro-Wilks test for normality&lt;/li&gt;
      &lt;li&gt;Kruskal-Wallis non-parametric test for group differences&lt;/li&gt;
      &lt;li&gt;Dunn post-hoc testing for pairwise comparisons&lt;/li&gt;
      &lt;li&gt;Box-cox for transforming skewed distribution to a gaussian&lt;/li&gt;
    &lt;/ul&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;packages&#34;&gt;Packages&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;Beautiful Soup&lt;/li&gt;
  &lt;li&gt;Scipy&lt;/li&gt;
  &lt;li&gt;Matplotlib&lt;/li&gt;
  &lt;li&gt;Pandas&lt;/li&gt;
  &lt;li&gt;Numpy&lt;/li&gt;
  &lt;li&gt;Scikit&lt;/li&gt;
  &lt;li&gt;StatsModels&lt;/li&gt;
&lt;/ol&gt;


&lt;/div&gt;




		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

</description>
    </item>
    
    <item>
      <title>Sal&#39;s Sanctuary</title>
      <link>https://arsakhar.github.io/project/sals-sanctuary/</link>
      <pubDate>Fri, 05 Jun 2020 22:19:19 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/sals-sanctuary/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;The first in a trilogy of virtual reality exergames designed to enhance brain health. Your adventure begins as a rookie ranger at Sal&amp;rsquo;s Sanctuary, a nature reserve for animals set in an urban environment. The animal&amp;rsquo;s have escaped the sanctuary and you are tasked with locating and returning them safely. Your selective attention and spatial navigation abilities will be tested each time you conduct a rescue mission.&lt;/p&gt;
&lt;p&gt;You are assigned to one route each day and will be required to make at least 4 search and rescue trips along this route. Once an animal is located, you will pick it up and bring it back to the gates of the sanctuary before starting your next search and rescue. You are also required to pick up any food along the route that is associated with the diet of the animal you have rescued.&lt;/p&gt;
&lt;p&gt;For the first two rescue missions, we will provide visual guidance to help you learn the route. After 2 rescue missions, you are required to remember the route using only the landmarks and environment as your guidance. At the beginning of each new day, prior to starting your 4 rescue missions, you will be required to remember and navigate the route from the previous day. The difficulty of your routes will increase and the routes will get longer as you start rescuing animals located farther away from the sanctuary.&lt;/p&gt;
&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/kI0bbrvH4hU&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Underwater Adventures</title>
      <link>https://arsakhar.github.io/project/underwater-adventures/</link>
      <pubDate>Fri, 05 Jun 2020 21:05:12 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/underwater-adventures/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;This is the third in our trilogy of virtual reality exergames. After conquering the land, you now turn your attention to the sea. Sal plans to build a marine sanctuary and he needs your help. On this adventure, you will take pictures and collect objects that will inspire the underwater sanctuary. You will start by exploring the island and collecting the equipment necessary to begin your descent into the ocean. Sal is planning on sending you pretty deep &amp;ndash; you didn&amp;rsquo;t hear it from me, but I heard he has some secret information on the lost city of Atlantis. We shouldn&amp;rsquo;t speculate, though.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Stay tuned for more to come&amp;hellip;&lt;/b&gt;&lt;/p&gt;
&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/UGuaEmGRSRE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Wildlife Enclosure</title>
      <link>https://arsakhar.github.io/project/wildlife-enclosure/</link>
      <pubDate>Fri, 05 Jun 2020 21:05:12 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/wildlife-enclosure/</guid>
      <description>&lt;h3 id=&#34;overview&#34;&gt;Overview&lt;/h3&gt;
&lt;p&gt;This is the second in our trilogy of virtual reality exergames. You start the game as the chief ranger at Sal&amp;rsquo;s Wildlife Enclosure, a nature reserve for animals set in a savannah, jungle, safari, and aviary. You are tasked with taking care of all the animal&amp;rsquo;s in the wildlife enclosure. Your missions consist of feeding animals, treating injured animals at the vet clinic, putting out fires, and repairing broken equipment. There are two primary cognitive skills assessed each day you bike around the enclosure to complete your missions: selective attention and spatial navigation.&lt;/p&gt;
&lt;p&gt;For the selective attention task, you are required to pick up objects associated with your mission. For example, if you are putting out a fire, you will be required to pickup axes and fire extinguishers on the way. For spatial navigation, you are assigned to one route each day. You will complete 4 missions, each time biking along the same route. For the first two missions, we will provide arrows to help you learn the route. After 2 missions, you are required to remember the route using only the landmarks and environment as cues. At the beginning of each day, prior to starting your 4 missions, you will be required to remember and navigate the route from the previous day. The difficulty of your routes will increase and the routes will get longer as you start moving towards the perimeter of the wildlife enclosure.&lt;/p&gt;
&lt;h3 id=&#34;video&#34;&gt;Video&lt;/h3&gt;
&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/YLXAhpwOIpM&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;iframe&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Meningioma Classification using Machine Learning</title>
      <link>https://arsakhar.github.io/project/meningioma/</link>
      <pubDate>Sun, 03 Feb 2019 15:30:47 -0700</pubDate>
      <guid>https://arsakhar.github.io/project/meningioma/</guid>
      <description>&lt;div class=&#34;article-container-nav&#34;&gt;

	&lt;nav&gt;
	
		&lt;div class=&#34;sticky-scroll navtab-position nav flex-row nav-pills&#34; id=&#34;nav-tab&#34; role=&#34;tablist&#34;&gt;
			&lt;br&gt;
			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link active&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab11&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;true&#34;&gt;Objective&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab12&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Data Acquisition&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab13&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Pre-Processing&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab14&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Training&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab15&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Results&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab16&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Discussion&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab17&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;References&lt;/a&gt;

			
			

			
			
			

			

			&lt;a class=&#34;nav-item nav-link&#34; id=&#34;nav-1&#34; data-toggle=&#34;tab&#34; href=&#34;#tab18&#34; role=&#34;tab&#34;
				 aria-controls=&#34;nav-home&#34; aria-selected=&#34;false&#34;&gt;Skills&lt;/a&gt;

			
			
		&lt;/div&gt;
	&lt;/nav&gt;

	&lt;div class=&#34;article-style &#34;&gt;
		&lt;div class=&#34;tab-content&#34; id=&#34;1&#34;&gt;
			






&lt;div class=&#34;tab-pane fade show active&#34; id=&#34;tab11&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName1&#34;&gt;

	&lt;h4 id=&#34;problem-statement&#34;&gt;Problem Statement&lt;/h4&gt;
&lt;p&gt;Meningiomas are tumors that form on the meninges, a membrane that covers the brain and spinal cord. The current standard of treatment is a craniotomy where the skull is opened to provide access to the tumor. Recent technological advances have allowed meningioma’s to be resected using minimally invasive endoscopic or key-hole surgical approaches.&lt;sup&gt;1&lt;/sup&gt; The selection of patients for these minimally invasive surgeries are based on several tumor characteristics such as pathology, vascularity, and invasiveness.&lt;sup&gt;1&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;One key characteristic that dictates ease of operation is tumor consistency, defined as texture or firmness.&lt;sup&gt;1&lt;/sup&gt; Tumor consistency, which can only be measured after surgical resection, is graded on a 5-point hardness scale (1-softest, 5-hardest).&lt;sup&gt;1,2&lt;/sup&gt; Training a machine learning model to classify a tumor according to its hardness using neuroimaging data could help physicians determine if a patient is a candidate for minimally invasive surgery.&lt;/p&gt;
&lt;h4 id=&#34;objective&#34;&gt;Objective&lt;/h4&gt;
&lt;p&gt;To train a convolutional neural network to classify meningioma tumors based on hardness ratings.&lt;/p&gt;
&lt;p&gt;Code can be downloaded at: &lt;a href=&#34;https://github.com/arsakhar/ML-Meningioma-Classification&#34;&gt;&lt;a href=&#34;https://github.com/arsakhar/ML-Meningioma-Classification&#34;&gt;https://github.com/arsakhar/ML-Meningioma-Classification&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab12&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName2&#34;&gt;

	&lt;h4 id=&#34;data-acquisition&#34;&gt;Data Acquisition&lt;/h4&gt;
&lt;p&gt;MRI scans were acquired on 85 patients across several hospitals. For each patient, an ADC, T2 flair, T1, T1 with contrast (T1c), and T2 scan were acquired. A lesion mask was manually created by the physician after image acquisition. Resected meningioma’s were manually labeled using a 0-3 hardness scale by expert raters.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/data-acquisition.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab13&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName3&#34;&gt;

	&lt;h4 id=&#34;pre-processing&#34;&gt;Pre-Processing&lt;/h4&gt;
&lt;p&gt;A supervised learning task for multi-class classification was considered based on the provided labels and discrete grading scale. A convolutional neural network classifier was chosen as the inputs were images.&lt;/p&gt;
&lt;p&gt;The bar chart below shows the distribution of meningioma classes for the acquired data. Class 0 and 1 were the lowest and highest frequency, respectively. Overall, the dataset was highly imbalanced between classes 1, 3 and 1,2. Moreover, only a limited sample of 425 scans (85 patients and 5 scans per patient) were available for training the model.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/data-exploration.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;One issue with the acquired data was that the dimensions of the scans were consistent within a subject but differed between subjects. Briefly, an MRI scan is a 3D volume of height, width, and depth where depth represents the number of slices.&lt;/p&gt;
&lt;p&gt;To ensure a consistent input shape into the CNN, each scan was resampled to a size of 256x256xZ. Z was set to 256 if the image had less than 256 slices. Otherwise, the original number of slices was retained.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/pre-processing-1.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;To increase the number of available training samples, each scan was decomposed into its individual slices.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/pre-processing-2.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;The figure below illustrates multiple slices across a single scan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/pre-processing-3.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Each individual slice was then re-assigned the original class label only if it contained the lesion based on the lesion mask. Slices that did not contain a lesion were not included in training the model. Decomposing and re-assigning class labels increased the accuracy of the label assignments as a single label was no longer applied to an entire 3D volume.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/pre-processing-4.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab14&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName4&#34;&gt;

	&lt;h4 id=&#34;training&#34;&gt;Training&lt;/h4&gt;
&lt;p&gt;Residual Neural Network 152 (ResNet 152) was selected as the CNN model for training. The following hyperparameters were randomly selected and evaluated using Optuna:&lt;/p&gt;
&lt;ul style=&#34;list-style-type:square;&#34;&gt;
&lt;li&gt;Optimizer – Adam, RMSprop, SGD&lt;/li&gt;
&lt;li&gt;Learning Rate – log uniform distribution, [1e-6, 1e-1]&lt;/li&gt;
&lt;li&gt;Dropout – uniform distribution, [0.1, 0.7]&lt;/li&gt;
&lt;li&gt;Batch Size – categorical, [16, 32, 64, 128]&lt;/li&gt;
&lt;li&gt;L2 Regularization – log uniform distribution, [1e-10, 1e-3]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The remaining training parameters were set as follows:&lt;/p&gt;
&lt;ul style=&#34;list-style-type:square;&#34;&gt;
&lt;li&gt;Input Channels – 5 [t1, t1c, t2 flair, t2, adc]&lt;/li&gt;
&lt;li&gt;# Classes – 4&lt;/li&gt;
&lt;li&gt;# Epochs – 45&lt;/li&gt;
&lt;li&gt;Loss function – cross-entropy&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The cross-entropy loss function was weighted according to the distribution of classes in the dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/training-1.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Two modifications were made to the original ResNet152 model. The first convolutional layer was modified to accept 5 input channels instead of 3 and the fully connected layer was modified to output probabilities for 4 classes instead of 1000.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/training-2.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;The ResNet 152 model was trained using 5-fold cross validation. The folds were stratified by subject to ensure all slices associated with a single subject were constrained to a single fold. Briefly, neighboring slices within a scan are structurally similar as they are anatomically acquired a few mm apart. This problem is exacerbated in subjects where the scan is up sampled to 256 slices during preprocessing. The similarity of slices results in data leakage if the slices are spread across the training and test fold. The folds were also stratified by class label to ensure the distribution of classes in the training and test fold approximated the actual distribution of classes for the acquired data.&lt;/p&gt;
&lt;p&gt;A custom dataset class was used to load, augment, and transform the slices to tensors for input into the CNN. TorchIO, a python package for medical image preprocessing, was used for data augmentation. Briefly, both training and validation slices were normalized to a mean of zero and standard deviation of one. Training slices were also augmented with a random blur, noise, bias field, motion, spike, ghosting, affine transformations, and elastic deformations at specified probabilities. This was done to provide additional unique images for the CNN to train on in the absence of a large training dataset.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/training-3.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;Each epoch consisted of a training and validation iteration. Both iterations consisted of a forward pass. Loss was calculated using the SoftMax output and target labels. The weights and bias in each layer were updated via back propagation for the training iterations. The argmax of the SoftMax output probabilities was used to calculate accuracy, defined by the proportion of correct predictions to total predictions.&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab15&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName5&#34;&gt;

	&lt;h4 id=&#34;results&#34;&gt;Results&lt;/h4&gt;
&lt;p&gt;Overall, the trained CNN model performed poorly on the validation dataset across multiple sets of hyperparameters selected by Optuna. The figure below shows the training and validation loss accuracy for a single fold and set of hyperparameters. As expected for training, the loss decreased (solid black line) and accuracy (dashed black line) increased, with accuracy approaching 95% after 45 epochs. However, validation loss (solid red line) and validation accuracy (dashed red line) did not improve during training. Validation loss decreased for the first 10 epochs before increasing. The validation accuracy increased to approximately 35% after 10 epochs before decreasing to 20%. Therefore, overfitting likely occurred after the 10th epoch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/results-1.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;
&lt;p&gt;The confusion matrix, calculated as a normalized sum of the confusion matrices for each k-fold, can be seen below. Of the meningiomas with an actual label of zero, the model correctly labeled 17%, while incorrectly labeling 34% and 47% as one and two, respectively. Of the meningiomas with an actual label of three, the model correctly labeled 12%, while incorrectly labeling 18% and 65% as one and two, respectively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://arsakhar.github.io/project/meningioma/results-2.png&#34; /&gt;&lt;/img&gt;&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab16&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName6&#34;&gt;

	&lt;h4 id=&#34;discussion&#34;&gt;Discussion&lt;/h4&gt;
&lt;p&gt;Overall, the CNN was unable to classify Meningioma’s with high accuracy and generalized very poorly to unseen validation data. The highest validation accuracy across the training folds was only 35% which is marginally better than 25% due to random guessing. The poor validation accuracy was seen for multiple combinations of hyperparameters decreasing the likelihood that the results were due to a poorly optimized model.&lt;/p&gt;
&lt;p&gt;Poor model performance may be due a lack of discernible patterns between classes of meningioma observed on an MRI. Indeed, it is possible that the biological properties underlying the tactile hardness, texture, and consistency differences observed by the expert rater during grading are not detectable on an MRI. It is also possible that inter-rater and intra-rater variability resulted in misclassification of some meningioma’s during labeling. This variability could affect model training as the labels are not representative of the actual ground truth.&lt;/p&gt;
&lt;p&gt;Despite the poor overall accuracy, the model was able to discriminate between the extreme classes, zero and three, relatively well. As seen in the confusion matrix, the model rarely assigned class three to class zero and vice versa. Taken together, this suggests that there may be a detectable pattern between classes.&lt;/p&gt;
&lt;h4 id=&#34;future-directions&#34;&gt;Future Directions&lt;/h4&gt;
&lt;p&gt;Due to computational requirements, the grid search for the optimal hyperparameters was only run for 3 iterations. Furthermore, only the ResNet 152 CNN was trained. Training additional CNN’s and continuing the grid search over 10-15 iterations may yield a trained model better suited for the provided dataset. Different data augmentation techniques may also improve model training as well. However, these changes will likely only result in a marginal improvement in classification accuracy. To evaluate whether Meningioma’s acquired by MRI can be classified with high accuracy, additional training data is needed.&lt;/p&gt;
&lt;p&gt;In the absence of additional training data, one possibility is to run an unsupervised machine learning technique such as k-means clustering over a range of clusters. A scree plot or silhouette analysis can then be used to identify the optimal number of clusters. The labels of the images in each cluster can then be compared to identify the purity of the cluster. If the optimal number of clusters is 4, it would suggest there is a detectable pattern aligned with the expected number of classes. Moreover, if the incorrectly labeled images in each cluster are better suited for the nearest neighboring cluster. it may suggest that inter-rater and intra-rater variability are contributing to problems with model training.&lt;/p&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab17&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName7&#34;&gt;

	&lt;h5 id=&#34;references&#34;&gt;References&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;Shiroishi, Mark S., et al. &amp;ldquo;Predicting meningioma consistency on preoperative neuroimaging studies.&amp;rdquo; Neurosurgery Clinics 27.2 (2016): 145-154.&lt;/li&gt;
&lt;li&gt;Zada, Gabriel, et al. &amp;ldquo;A proposed grading system for standardizing tumor consistency of intracranial meningiomas.&amp;rdquo; Neurosurgical focus 35.6 (2013): E1.&lt;/li&gt;
&lt;/ol&gt;


&lt;/div&gt;










&lt;div class=&#34;tab-pane fade&#34; id=&#34;tab18&#34; role=&#34;tabpanel&#34; aria-labelledby=&#34;nav-tabName8&#34;&gt;

	&lt;h4 id=&#34;technical-skills&#34;&gt;Technical Skills&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Pre-processing MRI data&lt;/li&gt;
&lt;li&gt;Manipulating and training a convolutional neural network for image classification&lt;/li&gt;
&lt;li&gt;Cross-validation for small datasets&lt;/li&gt;
&lt;li&gt;Generating classification reports and confusion matrices&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;packages&#34;&gt;Packages&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;Optuna&lt;/li&gt;
&lt;li&gt;NiBabel&lt;/li&gt;
&lt;li&gt;Pandas&lt;/li&gt;
&lt;li&gt;NumPy&lt;/li&gt;
&lt;li&gt;Torch&lt;/li&gt;
&lt;li&gt;Torchvision&lt;/li&gt;
&lt;li&gt;Scikit-learn&lt;/li&gt;
&lt;li&gt;TorchIO&lt;/li&gt;
&lt;/ol&gt;


&lt;/div&gt;





		&lt;/div&gt;
	&lt;/div&gt;
&lt;/div&gt;

</description>
    </item>
    
  </channel>
</rss>
